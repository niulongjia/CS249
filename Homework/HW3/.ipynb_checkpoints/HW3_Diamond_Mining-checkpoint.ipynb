{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CS249 -- Spring 2016 -- D.S. Parker &copy; 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3 -- Diamond Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-width:50px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Overview\n",
    "\n",
    "The goal of this assignment is for you to develop models for the <a href=\"http://docs.ggplot2.org/current/diamonds.html\"><b>diamonds</b> dataset</a>,\n",
    "which is included in the <a href=\"http://ggplot2.org\">ggplot2</a> package.\n",
    "\n",
    "This is a very simple assignment:  you are asked to build four models:\n",
    "LDA or QDA, simple Linear Regression, log-scaled Linear Regression, and Logistic Regression.\n",
    "You then just upload the formulas (R commands) you used to construct these models to CCLE.\n",
    "\n",
    "<hr style=\"border-width:20px;\">\n",
    "\n",
    "### Step 0:  build the numeric.diamonds dataset\n",
    "\n",
    "This notebook includes commands for buiding a dataset called `<code>numeric.diamonds</code>\n",
    "that you are to use for this assignment.\n",
    "\n",
    "\n",
    "The diamonds dataset has 3 categorical attributes (cut, color, clarity)\n",
    "that are <b>ordered</b>.\n",
    "<i>\n",
    "The <tt>numeric.diamonds</tt> dataset \n",
    "is a numeric version of the diamonds dataset\n",
    "in which all these categorical attributes are converted to integer codes.\n",
    "</i>\n",
    "\n",
    "For example, there are 7 colors, with the ordering: J < I < H < G < F < E < D (J is worst, D is best).\n",
    "We implement these by replacing J with the value 1,\n",
    "I with the value 2, ..., and D with the value 7.\n",
    "\n",
    "After doing this transformation for cut and clarity also,\n",
    "the result is an entirely numeric dataset called <tt>numeric.diamonds</tt>.\n",
    "\n",
    "In addition to this notebook, we've provided another called <tt>Diamonds.ipynb</tt> for gaining intuition\n",
    "about the data by walking through some exploratory graphics.\n",
    "Many aspects of the dataset are displayed.\n",
    "You do not have to use this notebook, it is totally optional, but it is included\n",
    "since visualization can help.\n",
    "\n",
    "<hr style=\"border-width:20px;\">\n",
    "\n",
    "### Step 1:  build a training set and test set (as subsets of numeric.diamonds) -- using your UID\n",
    "\n",
    "First, set the random number generator seed to your UID.\n",
    "Then generate a training set and test set using the following commands:\n",
    "\n",
    "<code>\n",
    "       MY_UID = 123456789 </code><b style=\"color:blue;\">########## you must enter your UCLA UID here !!!</b><code>\n",
    "       set.seed( MY_UID )\n",
    "\n",
    "       n = nrow( numeric.diamonds )\n",
    "       sample.size = 0.75 * n   ###### Use 75% of the data for the training set\n",
    "       training.row.ids = sample( (1:n), sample.size )\n",
    "       \n",
    "       my.training.set = numeric.diamonds[  training.row.ids, ]\n",
    "       my.test.set     = numeric.diamonds[ -training.row.ids, ]   # set complement of training.set.ids\n",
    "</code>\n",
    "\n",
    "<b>\n",
    "Please use exactly these commands to construct your training set and test set.\n",
    "Also, use the training set to construct each model,\n",
    "and use the test set to compute the accuracy of each model.\n",
    "The grading program will re-compute your model and its accuracy using this method.\n",
    "</b>\n",
    "\n",
    "<hr style=\"border-width:20px;\">\n",
    "\n",
    "### Step 2: compute accuracy of 4 Baseline Models about diamonds\n",
    "\n",
    "<hr style=\"border-width:20px;\">\n",
    "\n",
    "### Step 2: build four models about diamonds\n",
    "\n",
    "For the numeric.diamonds dataset you are to develop a notebook that builds four models in R:\n",
    "<ol><li>\n",
    "a LDA or QDA classification model that predicts a diamond's <b>Cut</b>.\n",
    "</li><li>\n",
    "a linear regression model that predicts a diamond's <b>Price</b>.\n",
    "</li><li>\n",
    "a linear regression model that predicts a diamond's <b>log10(Price)</b>.\n",
    "</li><li>\n",
    "a logistic regression model that predicts whether a diamond's <b>Price is above &dollar;1500</b>.\n",
    "</li></ol>\n",
    "\n",
    "As an example, you might produce these models:\n",
    "<ol><li>\n",
    "<code>  qda( cut ~ price + table + color + clarity,       data=my.training.set )</code>\n",
    "</li><li>\n",
    "<code>  lm(  price ~ carat + x + y + z + clarity,         data=my.training.set )</code>\n",
    "</li><li>\n",
    "<code>  lm(  log10(price) ~ table + log10(carat) + color, data=my.training.set )</code>\n",
    "</li><li>\n",
    "<code>  glm( I(price>1500) ~ carat + table + clarity,     data=my.training.set, family = binomial )</code>\n",
    "</li></ol>\n",
    "\n",
    "As these examples show, details matter:\n",
    "<b>you must specify the complete formula for each model in detail, listing all variables included.</b>\n",
    "\n",
    "Please choose attributes that produce the most accuracy models you can.\n",
    "More accurate models will get a higher score; see below.\n",
    "\n",
    "<hr style=\"border-width:20px;\">\n",
    "\n",
    "### Step 3: generate a CSV file \"HW3_output.csv\" including your 4 models\n",
    "\n",
    "If these were your four models, then to complete the assignment you would create\n",
    "a CSV file <tt>HW3_output.csv</tt> containing eight lines:\n",
    "\n",
    "<code>\n",
    "      33.333, qda( cut ~ .,           data=my.training.set )\n",
    "      88.888, lm(  price ~ .,         data=my.training.set )\n",
    "      77.777, lm(  log10(price) ~ .,  data=my.training.set )\n",
    "      88.888, glm( I(price>1500) ~ ., data=my.training.set, family=binomial )\n",
    "      44.444, qda( cut ~ price + table + color + clarity, data=my.training.set )\n",
    "      99.999, lm( price ~ carat + x + y + z + clarity, data=my.training.set )\n",
    "      99.999, lm( log10(price) ~ table + log10(carat) + color, data=my.training.set )\n",
    "      99.999, glm( I(price>1500) ~ carat + table + clarity, data=my.training.set, family=binomial )\n",
    "</code>\n",
    "\n",
    "<b>Each line gives the accuracy of a model <u>on <tt>my.test.set</tt></u></b>\n",
    "as well as <b>the exact command you used to generate the model</b>.\n",
    "There is no length restriction on the lines.\n",
    "\n",
    "<hr style=\"border-width:20px;\">\n",
    "\n",
    "### Step 4: upload your CSV file and notebook to CCLE\n",
    "\n",
    "Finally, go to CCLE and upload:\n",
    "<ul><li>\n",
    "your output CSV file <tt>HW3_output.csv</tt>\n",
    "</li><li>\n",
    "your notebook file <tt>HW3_Diamond_Mining.ipynb</tt>\n",
    "</li></ul>\n",
    "\n",
    "We are not planning to run any of the uploaded notebooks.\n",
    "However, your notebook should have the commands you used in developing your models ---\n",
    "in order to show your work.\n",
    "As announced, all assignment grading in this course will be automated,\n",
    "and the notebook is needed in order to check results of the grading program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-width:50px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rules -- building accurate models\n",
    "\n",
    "### All evaluation of your models' accuracy will be done using your Training and Test data.\n",
    "\n",
    "As mentioned above, everyone will generate their own Training set and Test set\n",
    "<b style=\"color:blue;\">after setting the random number seed to their UID</b>.\n",
    "The measure of accuracy will be determined by your Test set.\n",
    "\n",
    "As a result, in this assignment everyone has their own accuracy objective function,\n",
    "defined by their UID.\n",
    "\n",
    "Also: if you do not set the seed with your UID, your accuracy measures will differ from the\n",
    "ones produced by the grading program.\n",
    "\n",
    "### Models must be constructed using the functions lda(), qda(), lm(), and glm()\n",
    "\n",
    "You must use the <tt>lm</tt>, <tt>glm</tt>, <tt>lda</tt> and <tt>qda</tt> functions to compute your models.\n",
    "Both <tt>lda()</tt> and <tt>qda()</tt> are in the <tt>MASS</tt> package.)\n",
    "To produce a logistic regression model, you must include the option <tt>family=binomial</tt>, or equivalently <tt>family=binomial(\"logit\")</tt>.\n",
    "\n",
    "However, you have complete control over which variables are used in your model,\n",
    "and over which transformations you apply to the variables.\n",
    "So many different models are possible.\n",
    "\n",
    "### \"Accuracy\" is a standard measure of performance for models\n",
    "\n",
    "For LDA and QDA, and for logistic regression,\n",
    "the accuracy measure is the percentage of correct classifications.\n",
    "For linear regressions a standard measure is $R^2$.\n",
    "You must implement these measures as described below.\n",
    "\n",
    "### The grading program will compare your model's accuracy against a baseline model, using your data\n",
    "\n",
    "The grading program will use these baseline models:\n",
    "<code>\n",
    "      qda( cut ~ .,           data=my.training.set )\n",
    "      lm(  price ~ .,         data=my.training.set )\n",
    "      lm(  log10(price) ~ .,  data=my.training.set )\n",
    "      glm( I(price>1500) ~ ., data=my.training.set, family=binomial )\n",
    "</code>\n",
    "\n",
    "For each of the four baseline models, you must do two things:\n",
    "<ol><li>\n",
    "(18 points):  compute the accuracy of the baseline model.\n",
    "</li><li>\n",
    "(7 points):  find a model that has higher accuracy than the baseline model.\n",
    "</li></ol>\n",
    "\n",
    "It is not difficult to get 4 &ast; 18 = 72 points on this assignment.\n",
    "However, 4 models that are better than the baseline will give 4 &ast; 25 = 100 points.\n",
    "\n",
    "### After grading, we will produce a list of models designed by the class\n",
    "\n",
    "Since accuracy of the models is part of the HW score, make the models as accurate as you can.\n",
    "Some of your score on this assignment will depend on the accuracy of your models.\n",
    "All models will be compiled into a list and distributed at the end, so everyone can see the models\n",
    "that were developed and how they compared.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-width:50px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation of the numeric.diamonds Dataset\n",
    "\n",
    "## Use this transformed dataset for building all Models in this assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      ": package 'ggplot2' was built under R version 3.2.5Warning message:\n",
      ": package 'ggbiplot' is not available (for R version 3.2.4 Revised)"
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error in library(ggbiplot): there is no package called 'ggbiplot'\n",
     "output_type": "error",
     "traceback": [
      "Error in library(ggbiplot): there is no package called 'ggbiplot'\n"
     ]
    }
   ],
   "source": [
    "# we need the ggplot2 package to get the \"diamonds\" dataset\n",
    "\n",
    "not.installed <- function(pkg) !is.element(pkg, installed.packages()[,1])\n",
    "\n",
    "if (not.installed(\"ggplot2\")) install.packages(\"ggplot2\", repos = \"http://cran.r-project.org\")\n",
    "library(ggplot2)\n",
    "    \n",
    "if (not.installed(\"ggbiplot\")) install.packages(\"ggbiplot\", repos = \"http://cran.r-project.org\")\n",
    "library(ggbiplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     carat               cut        color        clarity          depth      \n",
       " Min.   :0.2000   Fair     : 1610   D: 6775   SI1    :13065   Min.   :43.00  \n",
       " 1st Qu.:0.4000   Good     : 4906   E: 9797   VS2    :12258   1st Qu.:61.00  \n",
       " Median :0.7000   Very Good:12082   F: 9542   SI2    : 9194   Median :61.80  \n",
       " Mean   :0.7979   Premium  :13791   G:11292   VS1    : 8171   Mean   :61.75  \n",
       " 3rd Qu.:1.0400   Ideal    :21551   H: 8304   VVS2   : 5066   3rd Qu.:62.50  \n",
       " Max.   :5.0100                     I: 5422   VVS1   : 3655   Max.   :79.00  \n",
       "                                    J: 2808   (Other): 2531                  \n",
       "     table           price             x                y         \n",
       " Min.   :43.00   Min.   :  326   Min.   : 0.000   Min.   : 0.000  \n",
       " 1st Qu.:56.00   1st Qu.:  950   1st Qu.: 4.710   1st Qu.: 4.720  \n",
       " Median :57.00   Median : 2401   Median : 5.700   Median : 5.710  \n",
       " Mean   :57.46   Mean   : 3933   Mean   : 5.731   Mean   : 5.735  \n",
       " 3rd Qu.:59.00   3rd Qu.: 5324   3rd Qu.: 6.540   3rd Qu.: 6.540  \n",
       " Max.   :95.00   Max.   :18823   Max.   :10.740   Max.   :58.900  \n",
       "                                                                  \n",
       "       z         \n",
       " Min.   : 0.000  \n",
       " 1st Qu.: 2.910  \n",
       " Median : 3.530  \n",
       " Mean   : 3.539  \n",
       " 3rd Qu.: 4.040  \n",
       " Max.   :31.800  \n",
       "                 "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data(diamonds, package=\"ggplot2\")\n",
    "\n",
    "summary(diamonds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has the following columns:\n",
    "<table>\n",
    "<tr><td>   <b>carat</b></td><td>weight of the diamond in carats, rounded to an integer  (1 carat = 0.2 grams)</td></tr>\n",
    "<tr><td>   <b>cut</b></td><td>quality of the cut  {Fair, Good, VeryGood, Premium, Ideal}</td></tr>\n",
    "<tr><td>   <b>color</b></td><td>color code: &lbrace; J &lt; I &lt; H &lt; G &lt; F &lt; E &lt; D &rbrace;  (J is worst, D is best)</td></tr>\n",
    "<tr><td>   <b>clarity</b></td><td>clarity code: &lbrace; I1 &lt; SI1 &lt; SI2 &lt; VS1 &lt; VS2 &lt; VVS1 &lt; VVS2 &lt; IF &rbrace; (I1 is worst, IF is best)</td></tr>\n",
    "\n",
    "<tr><td>   <b>depth</b></td><td>total depth percentage  =  2*z/(x+y)</td></tr>\n",
    "<tr><td>   <b>table</b></td><td>width of top of diamond relative to widest point</td></tr>\n",
    "\n",
    "<tr><td>   <b>price</b></td><td>in US dollars</td></tr>\n",
    "\n",
    "<tr><td>   <b>x</b></td><td>Length in mm (numeric value between 0 and 6)</td></tr>\n",
    "<tr><td>   <b>y</b></td><td>Width  in mm (numeric value between 0 and 9)</td></tr>\n",
    "<tr><td>   <b>z</b></td><td>Depth  in mm (numeric value between 0 and 6)</td></tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "Caution:  the datset has skewed distributions.  Please check below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>53940</li>\n",
       "\t<li>10</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 53940\n",
       "\\item 10\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 53940\n",
       "2. 10\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 53940    10"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim(diamonds)  # not a tiny dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# low_prices = subset( diamonds$price, diamonds$price<5000 )\n",
    "\n",
    "# hist( low_prices, breaks=200, col=\"green\",\n",
    "#      main = \"diamond prices below $5000; notice the odd gap around 1500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# log-scaling the prices makes patterns clearer\n",
    "\n",
    "# hist( log10(diamonds$price), breaks=50, col=\"skyblue\",\n",
    "#      main=\"distribution of log10(price) looks like a mixture\" )\n",
    "\n",
    "# plot(sort(log10(diamonds$price)), pch=\".\", col=\"skyblue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic cleaning of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "diamonds = subset( diamonds, (x>0) & (y>0) & (z>0) )  #  There are actually some zero values, we omit them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare numeric encodings of \"ordered categorical\" values for Cut, Color, and Clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "colnames(diamonds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "( colors = levels(diamonds$color) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## The levels of Colors should have the reverse ordering\n",
    "##  { D > E > F > G > H > I > J }\n",
    "##  (J is worst, D is best)\n",
    "\n",
    "( levels(diamonds$color) = rev(colors) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "( cuts = levels(diamonds$cut) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## The levels of Cuts have the correct ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "( clarities = levels(diamonds$clarity) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## The levels of Clarity should have ordering\n",
    "##  { I1 < SI1 < SI2 < VS1 < VS2 < VVS1 < VVS2 < IF }\n",
    "##  (I1 is worst, IF is best)\n",
    "\n",
    "( levels(diamonds$clarity) = clarities[c(1,3,2,5,4,7,6,8)] )\n",
    "#  reorder the factor levels of 'clarity' so that they match the real ordering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the categorical values to integers -- using the unclass() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numeric.diamonds = transform( diamonds,\n",
    "                              cut = as.numeric(unclass(diamonds$cut)),\n",
    "                              color = as.numeric(unclass(diamonds$color)),\n",
    "                              clarity = as.numeric(unclass(diamonds$clarity))\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "levels(diamonds$cut)\n",
    "table( diamonds$cut, numeric.diamonds$cut )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "levels(diamonds$color)\n",
    "table( diamonds$color, numeric.diamonds$color )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "levels(diamonds$clarity)\n",
    "table( diamonds$clarity, numeric.diamonds$clarity )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect correlations among the Numeric Diamonds variables, as a check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "diamonds.correlation.matrix = cor( numeric.diamonds )\n",
    "\n",
    "round( diamonds.correlation.matrix, 2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Quick PCA of a sample of the data, to see if everything looks OK\n",
    "\n",
    "n = nrow(numeric.diamonds)\n",
    "sample.size = 2000\n",
    "\n",
    "sample.row.ids = sample( (1:n), sample.size )\n",
    "\n",
    "numeric.diamonds.sample = numeric.diamonds[sample.row.ids, ]\n",
    "\n",
    "diamonds.sample = diamonds[sample.row.ids, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numeric.diamonds.pca = prcomp(numeric.diamonds.sample, scale.=TRUE)\n",
    "## biplot( numeric.diamonds.pca, xlabs=rep(\".\",sample.size) )\n",
    "\n",
    "ggbiplot( numeric.diamonds.pca, var.scale = 1,\n",
    "          groups = diamonds$cut[sample.row.ids], ellipse = TRUE ) +\n",
    "          labs(title = \"Diamonds dataset:  principal components biplot, colored by cut\")\n",
    "\n",
    "# More visualization examples are in the notebook  Diamonds.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ggplot(data=diamonds, aes(x=log10(carat), y=log10(price), color=color)) + geom_smooth() +\n",
    "     ggtitle( \"log10(price) as a function of log10(carat), colored by diamond color\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ggplot(data=diamonds, aes(x=log10(carat), y=log10(price), color=clarity)) +\n",
    "     geom_smooth() +\n",
    "     ggtitle( \"different grades of clarity can impact accuracy of the model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### another interesting plot, suggesting impacts on linearity by cut\n",
    "# ggplot(data=diamonds, aes(x=log10(carat), y=log10(price), color=cut)) + geom_smooth() +\n",
    "#     ggtitle( \"log10(price) as a function of log10(carat), colored by cut\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-width:50px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1:  generate your Training Set and Test Set from numeric.diamonds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  Please use exactly the following statements to generate these things:\n",
    "\n",
    "\n",
    "set.seed( 123456789 ) ########## please enter your UCLA UID here !!!\n",
    "\n",
    "\n",
    "n = nrow(numeric.diamonds)\n",
    "\n",
    "training.sample.size = 0.75 * n  ###### Use 75% of the data for the training set\n",
    "\n",
    "training.row.ids = sample( (1:n), training.sample.size )\n",
    "       \n",
    "my.training.set = numeric.diamonds[  training.row.ids, ]\n",
    "my.test.set     = numeric.diamonds[ -training.row.ids, ]   # set complement of training.set.ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2:  compute Accuracy of the 4 Baseline Models on your Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, there are 4 Baseline Models:\n",
    "<code>\n",
    "      qda( cut ~ .,           data=my.training.set )\n",
    "      lm(  price ~ .,         data=my.training.set )\n",
    "      lm(  log10(price) ~ .,  data=my.training.set )\n",
    "      glm( I(price>1500) ~ ., data=my.training.set, family=binomial )\n",
    "</code>\n",
    "\n",
    "Develop procedures to compute their accuracy:\n",
    "one for classification models\n",
    "(like lda() and qda() in the MASS package),\n",
    "one for linear regression models (like lm()),\n",
    "and one for logistic regression models (like glm(family=binomial)).\n",
    "\n",
    "See the section <b>Measuring Accuracy of Models in this Assignment</b> below.\n",
    "\n",
    "\n",
    "Then: use your procedures to compute the accuracy of the Baseline Models on your Test Set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2:  build 4 Models improving on the Baseline Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1:  a LDA or QDA classification model that predicts a diamond's Cut.\n",
    "\n",
    "An example of a possible model is:\n",
    "<code>\n",
    "sample_m1  =  qda( cut ~ price + table + color + clarity,       data=my.training.set )\n",
    "</code>\n",
    "\n",
    "If this model outperforms the first Baseline Model, you are done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2:  a linear regression model that predicts Price.\n",
    "\n",
    "An example of a possible model is:\n",
    "<code>\n",
    "sample_m2  =  lm(  price ~ carat + x + y + z + clarity,         data=my.training.set )\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: a linear regression model that predicts log10(Price).\n",
    "\n",
    "An example of a possible model is:\n",
    "<code>\n",
    "sample_m3  =  lm(  log10(price) ~ table + log10(carat) + color, data=my.training.set )\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4:  a logistic regression model that predicts whether Price is above &dollar;1500.\n",
    "\n",
    "An example of a possible model is:\n",
    "<code>\n",
    "sample_m4  =  glm( I(price>1500) ~ carat + table + clarity,   data=my.training.set, family = binomial )\n",
    "</code>\n",
    "\n",
    "Notice that the values of <code>I(price>1500)</code> are always 0 or 1.\n",
    "This model's predictions will be <b>0</b> if the price is below &dollar;1500,\n",
    "and <b>1</b> if the price is above &dollar;1500, so the resulting values are \"binomial\".\n",
    "\n",
    "The dataset includes information on about 50 thousand diamonds.\n",
    "About 20 thousand have a price below &dollar;1500; and the others have a price above.\n",
    "\n",
    "Thus a model that always simply predicts prices above &dollar;1500 might be right about 60% of the time.\n",
    "Your job is to do better than this baseline rate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: generate a CSV file \"HW3_output.csv\" including your model results\n",
    "\n",
    "If these were your four models, then to complete the assignment you would create\n",
    "a CSV file <tt>HW3_output.csv</tt> containing eight lines:\n",
    "<code>\n",
    "      33.333, qda( cut ~ .,           data=my.training.set )\n",
    "      88.888, lm(  price ~ .,         data=my.training.set )\n",
    "      77.777, lm(  log10(price) ~ .,  data=my.training.set )\n",
    "      88.888, glm( I(price>1500) ~ ., data=my.training.set, family=binomial )\n",
    "      44.444, qda( cut ~ price + table + color + clarity, data=my.training.set )\n",
    "      99.999, lm( price ~ carat + x + y + z + clarity, data=my.training.set )\n",
    "      99.999, lm( log10(price) ~ table + log10(carat) + color, data=my.training.set )\n",
    "      99.999, glm( I(price>1500) ~ carat + table + clarity, data=my.training.set, family=binomial )\n",
    "</code>\n",
    "\n",
    "Each line gives <b>the accuracy of a model <u>on <tt>my.test.set</tt></u></b>,\n",
    "and also <b>the exact command you used to generate your model</b>.\n",
    "The first four lines are for the baseline models.\n",
    "The second four lines are your improvements.\n",
    "\n",
    "There is no length restriction on the lines; they can be as long as you want.\n",
    "These examples above are just examples,\n",
    "and they may not improve on the Baseline Models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: upload your CSV file and notebook to CCLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Upload the files HW3_output.csv <u>and</u> your Jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-width:50px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background Material\n",
    "\n",
    "## R Formulas, Models, and the General Linear Model:\n",
    "\n",
    "Chapter 11 of the <a href=\"https://cran.r-project.org/doc/manuals/R-intro.pdf\">R Introduction Manual</a>\n",
    "gives a good description of formulas, models, model updating (exploring alternative models), and the GLM.\n",
    "\n",
    "<hr style=\"border-width:20px;\">\n",
    "\n",
    "### R formulas:\n",
    "\n",
    "Sections 11.1 in Chapter 11 of the <a href=\"https://cran.r-project.org/doc/manuals/R-intro.pdf\">R Introduction Manual</a>\n",
    "hs a good description of formulas.  A few constructs are important for this assignment:\n",
    "<ul><li> <b>Including an Intercept</b>:\n",
    "<br/>\n",
    "<code>  y ~ x</code><br/>\n",
    "<code>  y ~ x + 1</code><br/>\n",
    "<code>  y ~ 1 + x</code><br/>\n",
    "An intercept is included by default,\n",
    "so all of these formulas correspond to the same simple linear regression model of <code>y</code> on <code>x</code>.\n",
    "<br/>\n",
    "</li><li> <b>Omitting the Intercept</b>:\n",
    "<br/>\n",
    "<code>  y ~  x - 1</code><br/>\n",
    "<code>  y ~  x + 0</code><br/>\n",
    "<code>  y ~  0 + x</code><br/>\n",
    "<code>  y ~ -1 + x</code><br/>\n",
    "These formulas are all equivalent,\n",
    "and correspond to the linear regression of <code>y</code> on <code>x</code> without an intercept.\n",
    "<br/>\n",
    "</li><li> <b>Log-transformed Variables</b>:\n",
    "<br/>\n",
    "<code>  log(y) ~ x</code><br/>\n",
    "Regression on <code>x</code> of the transformed variable <code>log(y)</code> (with an implicit intercept term).\n",
    "<br/>\n",
    "</li><li> <b>Integer Powers of a Variable</b>:\n",
    "<br/>\n",
    "<code>  y ~ x^2</code><br/>\n",
    "<code>  y ~ 1 + x + I(x^2)</code><br/>\n",
    "<code>  y ~ poly(x,2)</code><br/>\n",
    "regression of y on a quadratic polynomial of x.\n",
    "<u>The first two formulas are equivalent --\n",
    "the power <code>x^2</code> implicitly includes the powers below it.</u>\n",
    "The third formula looks very similar, but is not completely equivalent:\n",
    "it uses \"orthogonal\" polynomials (with no interaction between each other),\n",
    "while the first two formulas use explicit powers, and interactions between them are considered.\n",
    "<br/>\n",
    "Note that the expression  <code>I(x^2)</code> represents an \"insulated\" new variable\n",
    "whose values are squares of values of <code>x</code>.\n",
    "<!--\n",
    "<br/>\n",
    "</li><li> <b>Integer Powers of a Variable</b>:\n",
    "y ~ A*B\n",
    "y ~ A + B + A:B\n",
    "y ~ B %in% A\n",
    "y ~ A/B Two factor non-additive model of y on A and B. The first two specify the same\n",
    "crossed classification and the second two specify the same nested classification. In\n",
    "abstract terms all four specify the same model subspace.\n",
    "<br/>\n",
    "</li><li> <b>Integer Powers of a Variable</b>:\n",
    "y ~ (A + B + C)^2\n",
    "y ~ A*B*C - A:B:C\n",
    "Three factor experiment but with a model containing main effects and two factor\n",
    "interactions only. Both formulae specify the same model.\n",
    "<br/>\n",
    "</li><li> <b>Integer Powers of a Variable</b>:\n",
    "y ~ A * x\n",
    "y ~ A/x\n",
    "y ~ A/(1 + x) - 1\n",
    "Separate simple linear regression models of y on x within the levels of A, with\n",
    "different codings. The last form produces explicit estimates of as many different\n",
    "intercepts and slopes as there are levels in A.\n",
    "-->\n",
    "</li></ul>\n",
    "\n",
    "<hr style=\"border-width:20px;\">\n",
    "\n",
    "### Functions on R models:\n",
    "\n",
    "Sections 11.3 in Chapter 11 of the <a href=\"https://cran.r-project.org/doc/manuals/R-intro.pdf\">R Introduction Manual</a>\n",
    "also gives a good description of functions on models that one can use:\n",
    "<ul><li>\n",
    "<b>coef</b>(model) <br/>\n",
    "Extract the regression coefficient (matrix).\n",
    "Long form: coefficients(object).\n",
    "<br/>\n",
    "<br/>\n",
    "</li><li>\n",
    "<b>formula</b>(model) <br/>\n",
    "Extract the model formula.\n",
    "<br/>\n",
    "<br/>\n",
    "</li><li>\n",
    "<b>plot</b>(model) <br/>\n",
    "Produce four plots, showing residuals, fitted values and some diagnostics.\n",
    "<br/>\n",
    "<br/>\n",
    "</li><li>\n",
    "<b>predict</b>(model, newdata=data.frame) <br/>\n",
    "The data frame supplied must have variables specified with the same labels as the\n",
    "original. The value is a vector or matrix of predicted values corresponding to the\n",
    "determining variable values in data.frame.\n",
    "<br/>\n",
    "<br/>\n",
    "</li><li>\n",
    "<b>residuals</b>(model) <br/>\n",
    "Extract the (matrix of) residuals, weighted as appropriate.\n",
    "<br/>\n",
    "<br/>\n",
    "</li><li>\n",
    "<b>step</b>(model) <br/>\n",
    "Select a suitable model by adding or dropping terms and preserving hierarchies. The\n",
    "model with the smallest value of AIC (Akaikeâ€™s An Information Criterion) discovered\n",
    "in the stepwise search is returned.\n",
    "<br/>\n",
    "<br/>\n",
    "</li><li>\n",
    "<b>summary</b>(model) <br/>\n",
    "Print a comprehensive summary of the results of the regression analysis.\n",
    "<br/>\n",
    "<br/>\n",
    "</li><li>\n",
    "<b>vcov</b>(model) <br/>\n",
    "Returns the variance-covariance matrix of the main parameters of a fitted model\n",
    "object.\n",
    "</li></ul>\n",
    "\n",
    "<hr style=\"border-width:20px;\">\n",
    "\n",
    "### The General Linear Model:\n",
    "\n",
    "Sections 11.6 in Chapter 11 of the <a href=\"https://cran.r-project.org/doc/manuals/R-intro.pdf\">R Introduction Manual</a>\n",
    "includes a tutorial on the GLM, and options for the glm() function.\n",
    "\n",
    "Relevant for this assignment:\n",
    "<ul><li>\n",
    "<code>glm(x ~ y, family = gaussian )</code>\n",
    "    is equivalent to the usual linear regression model\n",
    "<code>lm( x ~ y )</code>    \n",
    "</li><li>\n",
    "<code>glm(x ~ y, family = gaussian(\"log\") )</code>\n",
    "    is equivalent to the log-linear model\n",
    "<code>lm( log(x) ~ y )</code>   \n",
    "</li><li>\n",
    "<code>glm(x ~ y, family = binomial )</code>\n",
    "    is the logistic regression model\n",
    "</li></ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-width:50px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example:  Building a Model (simple supervised learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This example, discussed in class, uses the MASS package for supervised learning of LDA models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct a training set and test set (from the Iris dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data(iris)\n",
    "\n",
    "n = nrow(iris)\n",
    "\n",
    "training.sample.size = floor( 0.75 * n )  ###### Use 75% of the data for the training set\n",
    "\n",
    "iris.ids = (1:n)\n",
    "\n",
    "training.set = sample( iris.ids, training.sample.size ) # Generate a random sample\n",
    "test.set     = iris.ids[-training.set]       # The set complement of training.set\n",
    "\n",
    "training.set\n",
    "\n",
    "# table(iris$Species)                  # Tabulate the number of each species\n",
    "# table(iris$Species[training.set])    # Tabulate species for the training set\n",
    "# table(iris$Species[test.set])        # Tabulate species for the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing an LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if (!(is.element(\"MASS\", installed.packages())))  install.packages(\"MASS\")\n",
    "library(MASS)\n",
    "\n",
    "LDA.model = lda( Species ~ ., data = iris, subset = training.set )\n",
    "LDA.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple accuracy calculation (for a classification model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = predict( LDA.model, subset=test.set, data=iris )\n",
    "\n",
    "incorrect.predictions  =  (predictions$class != iris$Species[test.set] )\n",
    "\n",
    "incorrect.ids <-  test.set[incorrect.predictions]\n",
    "\n",
    "# iris[ incorrect.ids , ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "confusion.matrix = table( iris$Species[test.set], predictions$class )\n",
    "confusion.matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accuracy = (training.sample.size - length(incorrect.ids)) / training.sample.size\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-width:50px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples:  Measuring Accuracy of Models in this Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy of an LDA or QDA model is the percentage of correct classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FILL THIS IN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# classification_accuracy = function( model, test.data, test.solutions ) {\n",
    "    # .......................................................\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classification_accuracy( LDA.model, iris[test.set, 1:4], iris[test.set,5])\n",
    "\n",
    "# should be about  0.97"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy of a Linear Regression model is its R<sup>2</sup> value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FILL THIS IN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# linear_regression_accuracy = function( model, test.data, test.solutions ) {\n",
    "    # .......................................................\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample.LR.model = lm( Petal.Length ~ Petal.Width, data = iris, subset= training.set )\n",
    "linear_regression_accuracy( sample.LR.model, iris[test.set,], iris$Petal.Length[test.set] )\n",
    "\n",
    "# should be about  0.93"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy of a Logistic Regression model is the percentage of correct classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FILL THIS IN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# logistic_regression_accuracy = function( model, test.data, test.solutions ) {\n",
    "    # .......................................................\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LRiris = transform( iris, Species = I(Species == 'Virginica') )\n",
    "\n",
    "head(LRiris)  # For logistic regression, the Species is converted to {0, 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample.LR.model = suppressWarnings( glm( Species ~ ., data = LRiris,\n",
    "                                        subset=training.set, family='binomial' ) )\n",
    "\n",
    "logistic_regression_accuracy( sample.LR.model, LRiris[test.set,], LRiris$Species[test.set] )\n",
    "\n",
    "#  should be close to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-width:50px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Accuracy of the Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "baseline_m1 = qda( cut ~ .,             data=my.training.set )\n",
    "baseline_m1\n",
    "cat(sprintf(\"\\nClassification Accuracy: %8.6f\\n\",\n",
    "            classification_accuracy(baseline_m1, my.test.set, my.test.set$cut )))\n",
    "\n",
    "baseline_m2 = lm(  price ~ .,           data=my.training.set )\n",
    "summary(baseline_m2)\n",
    "cat(sprintf(\"\\nLinear Regression Accuracy: %8.6f\\n\",\n",
    "            linear_regression_accuracy(baseline_m2, my.test.set, my.test.set$price )))\n",
    "\n",
    "baseline_m3 = lm(  log10(price) ~ .,    data=my.training.set )\n",
    "summary(baseline_m3)\n",
    "cat(sprintf(\"\\nLinear Regression Accuracy: %8.6f\\n\",\n",
    "            linear_regression_accuracy(baseline_m3, my.test.set, log10(my.test.set$price) )))\n",
    "\n",
    "baseline_m4 = glm( I(price>1500) ~ ., data=my.training.set, family=binomial )\n",
    "summary(baseline_m4)\n",
    "cat(sprintf(\"\\nLogistic Regression Accuracy: %8.6f\\n\",\n",
    "            logistic_regression_accuracy(baseline_m4, my.test.set, I(my.test.set$price>1500) )))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now: improve on the Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  The Baseline models:\n",
    "\n",
    "m1 = qda( cut ~ .,           data=my.training.set )\n",
    "m2 = lm(  price ~ .,         data=my.training.set )\n",
    "m3 = lm(  log10(price) ~ .,  data=my.training.set )\n",
    "m4 = glm( I(price>1500) ~ ., data=my.training.set, family = binomial )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-width:50px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples: Converting between categorical and numeric variables\n",
    "\n",
    "R uses a jargon for categorical values that some find confusing:\n",
    "<ul><li>\n",
    "a categorical variable is called a <b>factor</b>\n",
    "</li><li>\n",
    "the set of all possible factor values is called its <b>levels</b>\n",
    "</li><li>\n",
    "the levels of a factor are not the same thing as strings --- they are symbolic values.\n",
    "</li></ul>\n",
    "\n",
    "This may seem odd at first, but it is very useful, and it is easy to get used to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Levels of a factor\n",
    "\n",
    "levels(iris$Species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  Altering the names of levels (dangerous)\n",
    "\n",
    "levels(iris$Species) = c( 'Setosa', 'Versicolor', 'Virginica' )\n",
    "levels(iris$Species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Converting numeric values to levels:\n",
    "\n",
    "cut.factor = as.factor( numeric.diamonds$cut )\n",
    "levels(cut.factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Converting numeric values to levels:\n",
    "\n",
    "cut.factor.numeric = unclass( cut.factor )\n",
    "# cut.factor.numeric = as.numeric( cut.factor )    ## more or less equivalent\n",
    "\n",
    "unique( cut.factor.numeric )  # find all unique values (in a list of values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-width:50px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples: Transforming variables to make them more nearly Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ?transform\n",
    "\n",
    "transformed.diamonds  =  transform(\n",
    "                                   numeric.diamonds,\n",
    "                                   log10_carat = log10(carat)\n",
    "                                  )\n",
    "\n",
    "#  this example transform adds a log10_carat column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "opar = par(mfrow=c(2,1))\n",
    "\n",
    "data = numeric.diamonds$price\n",
    "\n",
    "hist( data, probability=TRUE, col=\"deepskyblue\", breaks=50)\n",
    "curve( dnorm(x,mean(data),sd(data)), col=\"red\", lwd=3, add=TRUE)\n",
    "abline( v=1500, col=\"green\", lwd=3 )\n",
    "mtext( \"green line:  price = 1500\" )\n",
    "           \n",
    "hist( log10(data), probability=TRUE, col=\"deepskyblue\", breaks=50)\n",
    "curve( dnorm(x,mean(log10(data)),sd(log10(data))), col=\"red\", lwd=3, add=TRUE)\n",
    "abline( v=log10(1500), col=\"green\", lwd=3)\n",
    "mtext( \"green line:  price = 1500\" )\n",
    "  \n",
    "par(opar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ggplot( data=diamonds, aes(x=log10(price), y=log10(table), color=cut)) +\n",
    "        geom_smooth() + ggtitle(\"This doesn't look very linear (for some cuts)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  Possibly of interest:\n",
    "#  Box-Cox transformations are a popular way of making variables closer to Gaussian\n",
    "\n",
    "# library(MASS)\n",
    "# help(boxcox)\n",
    "# example(boxcox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-width:50px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fyi: regsubsets() might be useful for improving regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if (!(is.element(\"leaps\", installed.packages())))  install.packages(\"leaps\")\n",
    "  \n",
    "library(leaps)\n",
    "\n",
    "#  regsubsets() generates regression models for subsets of sizes up to nvmax\n",
    "#  regsubsets( y ~ x, data=D, nbest=3 )  #  generates the 3 best models of each size\n",
    "\n",
    "# ?regsubsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### regsubsets() searches for the best subsets of the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rs = regsubsets( carat ~ ., data=numeric.diamonds, nbest=1, nvmax=5 )\n",
    "\n",
    "summary(rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rs.summary = summary(rs)\n",
    "str(rs.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cat('subset of variables included in each model:\\n#var')\n",
    "print(rs.summary$which * 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = nrow(rs.summary$which)\n",
    "for (i in 1:N) {\n",
    "    cat(sprintf('\\n----- model %d: ----------------------\\n', i))\n",
    "    print( coef(rs, i) )\n",
    "    cat(sprintf(\" R^2:  %7.3f\\n\", rs.summary$rsq[i] ))\n",
    "    cat(sprintf(\" BIC:    %8.4g\\n\", rs.summary$bic[i] ))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(rs.summary$rsq, type=\"l\", col=\"blue\",\n",
    "     xlab=\"# of variables in model\", ylab=\"R-squared of model\",\n",
    "     main=\"R^2 increases as the number of variables increases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(rs.summary$bic, type=\"l\", col=\"red\",\n",
    "     xlab=\"# of variables in model\", ylab=\"BIC of model\",\n",
    "     main=\"BIC decreases as the number of variables increases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ?summary.regsubsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.2.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
